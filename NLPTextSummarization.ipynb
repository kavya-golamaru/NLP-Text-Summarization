{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset from Colab\n",
        "\n",
        "XSum dataset in google drive folder is generated using this documentation: https://github.com/EdinburghNLP/XSum/blob/master/XSum-Dataset/README.md"
      ],
      "metadata": {
        "id": "k2WHkDsd2_BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown https://drive.google.com/uc?id=1qgnZ-_N60Wd8LBTID-JYTxo7AfTW6a_C\n",
        "\n",
        "!unzip -q -o xsum-extracts-from-downloads.zip\n",
        "!rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "GMA2L7uF3Gbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c798ed31-c30d-4f89-fc9c-bb3963b691da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qgnZ-_N60Wd8LBTID-JYTxo7AfTW6a_C\n",
            "To: /content/xsum-extracts-from-downloads.zip\n",
            "100% 276M/276M [00:01<00:00, 158MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading/Cleaning X-sum data"
      ],
      "metadata": {
        "id": "G9iclX74A3eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "punctuation_list = [char for char in '''!()-[]{};:'\"\\,<>./?@#$%^&*_~''']\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])    \n",
        "def preprocess_article_texts(texts):\n",
        "    tokenized_texts = []\n",
        "    i = 0\n",
        "    for text in nlp.pipe(texts):\n",
        "        tokenized_text = [token.text for token in text if not token.is_stop]\n",
        "        \n",
        "        # remove punctuation\n",
        "        tokenized_text = [token for token in tokenized_text if token not in punctuation_list]\n",
        "        #print(text)\n",
        "        #print(tokenized_text)\n",
        "        tokenized_texts.append(tokenized_text)\n",
        "        i += 1\n",
        "        if i % 10000 == 0:\n",
        "            print(i)\n",
        "    return tokenized_texts\n",
        "\n",
        "def preprocess_summary_texts(texts):\n",
        "    tokenized_texts = []\n",
        "    i = 0\n",
        "    for text in nlp.pipe(texts):\n",
        "        tokenized_text = [token.text for token in text]\n",
        "\n",
        "        tokenized_texts.append(tokenized_text)\n",
        "        i += 1\n",
        "        if i % 10000 == 0:\n",
        "            print(i)\n",
        "    return tokenized_texts"
      ],
      "metadata": {
        "id": "6h-U182r5Djd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading data\n",
        "import os\n",
        "data_dir = \"xsum-extracts-from-downloads/\"\n",
        "\n",
        "summary_list = []\n",
        "article_text_list = []\n",
        "data_files = os.listdir(data_dir)\n",
        "for i, data_file in enumerate(data_files):\n",
        "    if i < 1000:\n",
        "      with open(os.path.join(data_dir, data_file)) as file:\n",
        "          lines = file.readlines()\n",
        "          summary = lines[lines.index('[XSUM]INTRODUCTION[XSUM]\\n') + 1].replace('\\n', '').lower()\n",
        "\n",
        "          article_start_idx = lines.index('[XSUM]RESTBODY[XSUM]\\n') + 1\n",
        "          split_article_text = [line.replace('\\n', '').lower() for line in lines[article_start_idx:]]\n",
        "          if split_article_text[0] == 'share this with':\n",
        "              try:\n",
        "                  index = split_article_text.index('these are external links and will open in a new window')\n",
        "                  split_article_text = split_article_text[index + 1:]\n",
        "              except:\n",
        "                  try:\n",
        "                      index = split_article_text.index('copy this link')\n",
        "                      split_article_text = split_article_text[index + 1:]\n",
        "                  except:\n",
        "                      print(\"Couldn't find ending of article \" + str(i))\n",
        "                      print(split_article_text)\n",
        "          article_text = ' '.join(split_article_text)\n",
        "          summary_list.append(summary)\n",
        "          article_text_list.append(article_text)\n",
        "\n",
        "article_text_list = preprocess_article_texts(article_text_list)\n",
        "summary_list = preprocess_summary_texts(summary_list)\n",
        "print(len(summary_list), len(article_text_list))"
      ],
      "metadata": {
        "id": "8KaP0QkS5DAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abf2d33-4ae7-4c51-df3c-9fb8ecd29950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# create a vocabulary\n",
        "vocab_freq = {}\n",
        "for article_text in tqdm(article_text_list, position=0, leave=True):\n",
        "    for word in article_text:\n",
        "        if word in vocab_freq:\n",
        "            vocab_freq[word] += 1\n",
        "        else:\n",
        "            vocab_freq[word] = 1\n",
        "\n",
        "print(\"Number of words total: \" + str(len(vocab_freq)))\n",
        "words = np.array([])\n",
        "freqs = np.array([])\n",
        "\n",
        "for key, value in vocab_freq.items():\n",
        "    words = np.append(words, key)\n",
        "    freqs = np.append(freqs, value)\n",
        "\n",
        "index = np.argsort(freqs)[::-1]\n",
        "words = words[index]\n",
        "freqs = freqs[index]\n",
        "print(words)\n",
        "\n",
        "# take the top vocab_size freq words\n",
        "vocab_size = 1000\n",
        "words = words[:1000]\n",
        "\n",
        "# assign a new index to each word\n",
        "word_to_idx = {'[PAD]': 0, '[SOS]': 1, '[EOS]': 2, '[OOV]': 3}\n",
        "for i, word in enumerate(words):\n",
        "    word_to_idx[word] = i + 4\n",
        "idx_to_word = {value: key for key, value in word_to_idx.items()}"
      ],
      "metadata": {
        "id": "dRffnl8F9U3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4bc364-5414-40e9-db5a-ddbedf7ba0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 5333.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words total: 25303\n",
            "['said' 'mr' 'people' ... 'wean' 'â£19' 'succumbed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use word_to_idx to convert tokens to idx\n",
        "article_tokenized_list = []\n",
        "for i in range(len(article_text_list)):\n",
        "    article_tokenized_list.append([word_to_idx[word] if word in word_to_idx else word_to_idx['[OOV]'] for word in article_text_list[i]])\n",
        "print(article_tokenized_list[:10])\n",
        "\n",
        "summary_tokenized_list = []\n",
        "for i in range(len(summary_list)):\n",
        "    summary_tokenized_list.append([word_to_idx[word] if word in word_to_idx else word_to_idx['[OOV]'] for word in summary_list[i]])\n",
        "print(summary_tokenized_list[:10])"
      ],
      "metadata": {
        "id": "FbpQ56Oi5K8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27dd50b7-b05d-420b-861e-ac06a9d765af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[79, 319, 206, 229, 3, 725, 3, 598, 707, 3, 3, 3, 272, 261, 75, 3, 3, 3, 3, 3, 3, 119, 3, 3, 3, 793, 3, 609, 3, 68, 32, 7, 4, 3, 3, 17, 180, 3, 3, 524, 725, 3, 3, 738, 160, 3, 3, 3, 3, 3, 660, 3, 3, 3, 433], [3, 3, 3, 3, 111, 3, 137, 3, 3, 3, 3, 139, 67, 351, 68, 3, 78, 3, 3, 126, 987, 66, 903, 3, 3, 461, 554, 4, 3, 17, 344, 3, 3, 67, 344, 139, 3, 3, 3, 137, 3, 67, 139, 139, 45, 174, 71, 782, 137, 68, 3, 3, 3, 3, 3, 78, 47, 337, 3, 3, 149, 493, 3, 137, 496, 171, 461, 554, 4, 3, 111, 3, 428, 3, 3, 3, 653, 428, 51, 3, 3, 3, 194, 3, 370, 3, 3, 3, 3, 3, 3, 3, 20, 3, 78, 3, 3, 410, 289, 3, 3, 3, 428, 3, 3, 3, 3, 3, 3, 777, 3, 3, 3, 171, 544, 63, 3, 3, 3, 3, 420, 67, 3, 3, 3, 199, 3, 3, 420, 67, 4, 3, 3, 3, 67, 31, 3, 84, 3, 579, 193, 384, 3, 114, 541, 370, 156, 6, 3, 3], [3, 637, 3, 437, 116, 3, 151, 3, 3, 322, 3, 903, 765, 154, 3, 3, 3, 592, 685, 8, 210, 3, 3, 62, 3, 3, 3, 14, 3, 702, 3, 3, 3, 130, 927, 3, 151, 3, 110, 3, 29, 565, 279, 210, 3, 3, 3, 3, 3, 62, 469, 814, 3, 3, 3, 3, 3, 3, 187, 3, 3, 3, 814, 3, 3, 3, 790, 3, 36, 3, 3, 3, 3, 3, 925, 181, 702, 3, 3, 3, 702, 702, 3, 3, 14, 3, 702, 997, 130, 278, 3, 3, 3, 3, 3, 305, 3, 130, 3, 3, 3, 925, 181, 518, 3, 62, 3, 438, 3, 3, 3, 3], [312, 3, 3, 3, 935, 176, 3, 3, 221, 3, 3, 142, 3, 3, 3, 3, 3, 3, 4, 312, 3, 681, 129, 24, 71, 3, 3, 3, 455, 323, 3, 3, 3, 3, 3, 3, 312, 3, 3, 3, 217, 3, 73, 3, 3, 3, 3, 735, 3, 4, 3, 312, 3, 715, 915, 3, 3, 935, 518, 3, 3, 732, 3, 70, 3, 3, 3, 3, 436, 3, 3, 3, 3, 3, 3, 208, 3, 191, 214, 3, 3, 214, 214, 3, 34, 3, 480, 735, 3, 4, 518, 3, 3, 3, 3, 3, 518, 500, 3, 3, 27, 3, 3, 3, 3, 3, 3, 186, 735, 3, 4, 371, 50, 312, 3, 3, 3, 3, 327, 3, 3, 3, 3, 3, 801, 3, 253, 621, 480, 289, 4, 3, 492, 399, 3, 3, 312, 3, 274, 142, 3, 199, 3, 287, 380, 3, 3, 312, 3, 3, 3, 3, 3, 682, 3, 3, 3, 3, 3, 3, 3, 566, 3, 3, 214, 622, 735, 3, 4, 735, 3, 4, 312, 3, 3, 3, 914, 3, 70, 312, 3, 533, 3, 113, 3, 732, 3, 3, 3, 3, 3, 3, 3, 3, 318, 3, 3, 3, 3, 4, 62, 469, 3, 4, 318, 3, 3, 3, 914, 735, 3, 4, 3, 3, 3, 3, 924, 3, 62, 469, 312, 3, 3, 3, 3, 3, 142, 735, 3, 4, 3, 3, 3, 815, 197, 3, 3, 3, 3, 3, 3, 3, 3, 469, 318, 3, 381, 252, 3, 3, 426, 3, 32, 569, 3, 3, 3, 935, 3, 3, 3, 386, 298, 3, 4, 312, 3, 312, 3, 518, 500, 3, 122, 433, 935, 3, 400, 3, 50, 3, 3, 323, 4, 3, 593, 262, 3, 3, 3, 327, 7, 455, 323, 744, 3, 3, 304, 3, 801, 175, 3], [3, 426, 565, 3, 65, 3, 800, 24, 3, 3, 33, 20, 231, 161, 214, 3, 248, 355, 89, 192, 3, 3, 3, 3, 3, 49, 65, 3, 3, 3, 603, 14, 283, 660, 884, 65, 113, 3, 3, 3, 4, 65, 300, 3, 4, 412, 30, 775, 89, 3, 3, 308, 3, 390, 3], [75, 3, 3, 96, 3, 3, 92, 204, 3, 3, 92, 3, 874, 3, 3, 70, 212, 3, 1000, 190, 3, 3, 261, 515, 3, 3, 3, 3, 123, 3, 3, 3, 60, 103, 3, 3, 3, 526, 219, 111, 212, 3, 3, 3, 80, 3, 110, 3, 34, 69, 77, 170], [3, 834, 3, 3, 3, 3, 195, 3, 3, 3, 174, 545, 3, 3, 704, 139, 233, 3, 3, 3, 3, 509, 3, 81, 253, 3, 539, 3, 509, 3, 3, 3, 3, 260, 786, 233, 3, 49, 3, 3, 3, 3, 3, 3, 768, 3, 3, 3, 123, 3, 419, 3, 3, 3, 3, 166, 3, 3, 349, 214, 3, 3, 3, 3, 3, 385, 717, 267, 3, 3, 3, 416, 3, 253, 3, 704, 139, 3, 768, 553, 972, 487, 786, 3, 3, 3, 3, 3, 349, 3, 3, 3, 3, 786, 385, 3, 3, 3, 3, 800, 3, 704, 139, 3, 768, 487, 786, 3, 3, 3, 3, 775, 31, 3, 390, 255, 3, 3, 289, 178, 3, 880, 3, 3, 928, 3, 139, 704, 3, 3, 14, 40, 3, 466, 717, 3, 3, 31, 3, 3, 3, 36, 790, 215, 3, 290, 3, 3, 3, 3, 3, 643, 359, 3, 3, 3, 3, 3, 3, 3, 251, 704, 349, 242, 3, 910, 3, 233, 3, 585, 706, 3, 36, 3, 300, 3, 317, 3, 3, 260, 315, 3, 413, 3, 3, 420, 3, 3, 3, 238, 3, 588, 3, 3, 15, 17, 584, 3, 535, 64, 71, 129, 172, 578, 251, 3, 712, 300, 3, 251, 3, 3, 3, 3, 807, 3, 255, 193, 86, 588, 3, 3, 928, 3, 704, 3, 3, 3, 3, 519, 45, 3, 3, 3, 3, 3, 602, 786, 557, 3, 209, 193, 3, 3, 588, 317, 767, 253, 431, 3, 3, 3, 3, 3, 3, 376, 3, 3, 173, 139, 3, 476, 49, 6, 3, 312, 3, 51, 3, 3, 3, 972, 253, 3, 539, 834, 444, 466, 168, 769, 728, 3, 676, 3, 403, 487, 3, 63, 321, 3, 170, 3, 3, 444, 3, 933, 3, 3, 24, 57, 3, 794, 125, 17, 523, 3, 474], [60, 874, 3, 3, 3, 3, 133, 3, 133, 3, 997, 9, 22, 506, 3, 125, 976, 777, 655, 3, 10, 4, 3, 3, 3, 110, 3, 3, 404, 3, 4, 3, 185, 595, 3, 3, 110, 7, 347, 154, 133, 3, 3, 804, 3, 3, 3, 517, 184, 3, 185, 3, 6, 3, 263, 3, 3, 70, 3, 3, 371, 201, 3, 964, 401], [3, 776, 722, 3, 37, 3, 3, 8, 80, 3, 3, 3, 3, 3, 3, 4, 3, 297, 211, 97, 3, 3, 3, 3, 228, 3, 942, 320, 3, 3, 711, 3, 3, 3, 21, 37, 3, 3, 3, 3, 219, 94, 68, 3, 879, 543, 787, 514, 241, 3, 264, 711, 25, 3, 126, 3, 854, 21, 3, 801, 3, 3, 3, 37, 127, 870, 246, 9, 3, 65, 246, 9, 3, 3, 27, 3, 338, 514, 242, 37, 21, 3, 540, 33, 4, 3, 246, 819, 190, 391, 37, 3, 963, 658, 450, 3, 3, 3, 320, 3, 144, 3, 675, 3, 176, 353, 33, 664, 215, 37, 21, 127, 3, 563, 416, 3], [3, 3, 3, 3, 3, 11, 3, 3, 3, 3, 162, 170, 3, 3, 3, 19, 3, 19, 26, 3, 19, 9, 111, 138, 3, 198, 53, 3, 3, 3, 392, 3, 48, 3, 155, 3, 3, 3, 7, 588, 289, 3, 3, 155, 715, 3, 5, 3, 3, 149, 3, 256, 3, 3, 560, 3, 3, 4, 5, 3, 3, 3, 8, 3, 149, 3, 3, 3, 3, 149, 12, 3, 983, 3, 101, 3, 3, 193, 3, 3, 3, 603, 240, 3, 4, 3, 787, 574, 4, 3, 979, 8, 121, 3, 614, 987, 3, 379, 3, 680, 4, 3, 3, 321, 14, 304, 3, 3, 79, 3, 3, 221, 3, 3, 40, 3, 5, 3, 4, 941, 25, 574, 588, 289, 4, 5, 3, 3, 3, 690, 3, 102, 3, 3, 3, 3, 11, 203, 321, 14, 3, 3, 101, 3, 238, 3, 3, 3, 509, 27, 588, 289, 3, 3, 19, 3, 19, 26, 3, 19, 18, 3, 549, 3, 130, 601, 256, 7, 423, 9, 3, 61, 19]]\n",
            "[[114, 249, 3, 3, 3, 127, 3, 3, 3, 3, 3, 3, 3, 3, 180, 3, 3, 3, 3, 3], [3, 461, 554, 3, 3, 3, 196, 137, 3, 3, 496, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 78, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 409, 186, 3, 935, 112, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 73, 3, 62, 3, 469, 3], [3, 603, 3, 3, 3, 977, 3, 65, 3, 479, 79, 112, 264, 3, 94, 3, 165, 597, 3, 3, 3], [212, 3, 707, 3, 187, 3, 742, 808, 3, 68, 3, 3, 3, 92, 3, 3, 3, 7, 3, 3, 14, 3], [162, 3, 64, 3, 3, 3, 3, 3, 3, 105, 3, 3, 3, 3, 3, 3, 510, 3, 3, 3, 704, 139, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 178, 3, 3, 3, 3], [10, 3, 3, 3, 3, 3, 3, 3, 3, 22, 60, 3, 3, 3, 3, 3, 3, 3, 3, 777, 3, 358, 3], [3, 3, 3, 3, 3, 3, 3, 3, 167, 21, 3, 3, 161, 3, 3, 3], [588, 289, 3, 121, 332, 3, 3, 3, 310, 3, 3, 3, 3, 3, 3, 3, 3, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finds the length of the longest summary and the length of the longest article\n",
        "\n",
        "longestSummary = 0\n",
        "longestText = 0\n",
        "print(len(summary_list), len(article_text_list))\n",
        "for summary in summary_list:\n",
        "  longestSummary = max(longestSummary, len(summary))\n",
        "  if len(summary) == 55:\n",
        "    print(summary)\n",
        "for article_text in article_text_list:\n",
        "  longestText = max(longestText, len(article_text))\n",
        "  if len(article_text) == 16000:\n",
        "    print(article_text)\n",
        "#Longest summary = 410, longest text = 873\n",
        "print(longestSummary, longestText)"
      ],
      "metadata": {
        "id": "0c4SDP_-sIWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d5d2c0-fb03-4453-85dc-0146b8bd79d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 1000\n",
            "58 2010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make all summaries length 190 and all texts length 873\n",
        "\n",
        "#for summary in summary_list:\n",
        "  #while len(summary) < 55:\n",
        "   # summary.append(0)\n",
        "short_article_list = []\n",
        "for article_text in article_text_list:\n",
        "  short_article_list.append(article_text)\n",
        "print(len(article_text_list))\n",
        "\n",
        "print(article_text_list[1])"
      ],
      "metadata": {
        "id": "pQIVXY9m6KF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14723b0-bb24-4d0e-a870-7dd7439796e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "['hunter', 'foundation', 'donating', 'table', '10', 'secondary', 'school', 'enter', 'pupils', 'write', 'essay', 'change', 'scotland', 'return', 'win', 'table', 'president', 'lecture', 'edinburgh', 'international', 'conference', 'centre', '26', 'invitation', 'businessman', 'sir', 'tom', 'said', 'ruled', 'world', '...', 'oh', 'okay', 'scotland', '...', 'change', 'write', 'essay', 'summarises', 'school', 'ran', 'scotland', 'change', 'change', 'think', 'run', 'country', 'tell', 'school', 'win', 'table', 'dinner', 'barack', 'obama', '44th', 'president', 'united', 'states', 'address', 'philanthropy', 'business', 'leaders', 'dinner', 'school', 'chance', 'event', 'sir', 'tom', 'said', 'table', '10', 'offered', 'winning', 'pupil', 'select', 'table', 'member', 'winning', 'group', 'teacher', 'adult', 'permission', 'given', 'pupils', 'schools', 'enter', 'essays', 'maximum', 'pages', 'pupils', 'encouraged', 'creative', 'like', 'prize', 'president', 'obama', 'lecture', 'include', 'post', 'lecture', 'entertainment', 'oscar', 'winning', 'musician', 'annie', 'lennox', 'comedian', 'kevin', 'bridges', 'glasgow', 'band', 'texas', 'perform', 'event', 'competition', 'set', 'hunter', 'foundation', 'partnership', 'skills', 'development', 'scotland', 'founders4schools', 'danny', 'logue', 'director', 'operations', 'skills', 'development', 'scotland', 'said', 'sds', 'careers', 'advisers', 'scotland', 'work', 'pupils', 'support', 'strengths', 'potential', 'future', 'career', 'ambitions', 'great', 'opportunity', 'schools', 'young', 'people', 'creativity', 'innovation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset Class"
      ],
      "metadata": {
        "id": "u-Ltnjaw4T80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Based on https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, text, summaries):\n",
        "    self.text = text\n",
        "    self.summaries = summaries\n",
        "    self.tensors = None\n",
        "  \n",
        "# Length of dataset\n",
        "def __len__(self):\n",
        "  return len(self.summaries)\n",
        "\n",
        "# Retrieves a specific item from the dataset\n",
        "def __getItem__(self, idx):\n",
        "  summaryTensor = tf.Tensor(summaries[i], shape=(1, len(summaries[0])))\n",
        "  textTensor = tf.Tensor(text[i], shape=(1, len(text[0])))\n",
        "  return (summaryTensor, textTensor)\n",
        "\n",
        "# create Pandas DataFrame\n",
        "#text_labels_df = pd.DataFrame({'Text': text, 'Labels': summaries})\n",
        "# define data set object\n",
        "#ND = NewsDataset(text_labels_df['Text'], text_labels_df['Labels'])"
      ],
      "metadata": {
        "id": "nxTWUe7N4aEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model"
      ],
      "metadata": {
        "id": "JZAPQWGS4e_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    \n",
        "    # input: (batch_size, seq_length)\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input) # embedded: (batch_size, seq_length, input_size)\n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden # output: (batch_size, seq_length, hidden_size)\n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)"
      ],
      "metadata": {
        "id": "ZbqokJ5F4w0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # input: (batch_size, seq_length)\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input) # embedded: (batch_size, seq_length, input_size)\n",
        "\n",
        "        output = F.relu(embedded)\n",
        "        output, hidden = self.rnn(output, hidden) # output: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden # output: (batch_size, seq_length, vocab_size)\n",
        "        \n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)"
      ],
      "metadata": {
        "id": "UGTpPOrd9JdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test encoder-decoder shapes\n",
        "vocab_size = 100\n",
        "global_input_size = 30\n",
        "global_hidden_size = 50\n",
        "batch_size = 16\n",
        "\n",
        "encoder = EncoderRNN(vocab_size, global_input_size, global_hidden_size)\n",
        "decoder = DecoderRNN(vocab_size, global_input_size, global_hidden_size)\n",
        "\n",
        "encoder_input_tensor = torch.ones((batch_size, 80), dtype=int) # (batch_size, seq_length)\n",
        "encoder_out = encoder(encoder_input_tensor, encoder.initHidden(batch_size))\n",
        "print(\"encoder output shape: \" + str(encoder_out[0].shape))\n",
        "print(\"encoder hidden shape: \" + str(encoder_out[1].shape))\n",
        "print()\n",
        "\n",
        "decoder_input_tensor = torch.ones((batch_size, 80), dtype=int) # (batch_size, seq_length)\n",
        "decoder_out = decoder(decoder_input_tensor, encoder_out[1])\n",
        "print(\"decoder output shape: \" + str(decoder_out[0].shape))\n",
        "print(\"decoder hidden shape: \" + str(decoder_out[1].shape))"
      ],
      "metadata": {
        "id": "QaZ17_rwsJUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69fb5c1-0839-418e-98ad-8be6af711b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder output shape: torch.Size([16, 80, 50])\n",
            "encoder hidden shape: torch.Size([1, 16, 50])\n",
            "\n",
            "decoder output shape: torch.Size([16, 80, 100])\n",
            "decoder hidden shape: torch.Size([1, 16, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Training Loop"
      ],
      "metadata": {
        "id": "4tAD28oY4bm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = [word_to_inx[word] for word in sentence.split(' ')]\n",
        "    indexes.append(word_to_inx['EOS'])\n",
        "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(lang, pair):\n",
        "    input_tensor = tensorFromSentence(lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def generate_data_indexes(data_length):\n",
        "    indexes = [i for i in range(data_length)]\n",
        "    random.shuffle(indexes)\n",
        "\n",
        "    # 80:20:0 train validation test split\n",
        "    train_idx = int(data_length * 0.8)\n",
        "    val_idx = train_idx + int(data_length * 0.2)\n",
        "    return indexes[:train_idx], indexes[train_idx:val_idx], indexes[val_idx:]\n",
        "\n",
        "def trainIter(input_tensor, target_tensor, encoder, decoder,\n",
        "      encoder_optimizer, decoder_optimizer, criterion):\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    # ignorig encoder_output for now, will add attention later\n",
        "    # encoder_hidden stores final result from encoder\n",
        "    for ei in range(input_length):\n",
        "      encoder_output, encoder_hidden = encoder.forward(\n",
        "        input_tensor[ei], encoder_hidden)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = torch.tensor([[word_to_inx['SOS']]])\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder.forward(decoder_input,\n",
        "                                                            decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder.forward(decoder_input,\n",
        "                                                             decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "\n",
        "            topt, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            \n",
        "            if decoder_input.item() == word_to_inx['EOS']:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def train(encoder, decoder, pair_percent=0.1, print_every=500, learning_rate=0.01):\n",
        "\n",
        "    print('Training in progress...')\n",
        "\n",
        "    num_iters = int(pair_percent * len(training_indexes))\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(all_pairs[i])\n",
        "                      for i in training_indexes[:num_iters]]\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss_to_print = 0\n",
        "\n",
        "    for cur_iter in range(num_iters):\n",
        "        training_pair = training_pairs[cur_iter]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        cur_loss = trainIter(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        total_loss_to_print += cur_loss\n",
        "\n",
        "        if (cur_iter + 1) % print_every == 0:\n",
        "            avg_loss_to_print = total_loss_to_print / print_every\n",
        "            total_loss_to_print = 0\n",
        "            print('%d% done: avg_loss = %.4f' % ((cur_iter + 1) / num_iters * 100,\n",
        "                                                 avg_loss_to_print))\n",
        "\n"
      ],
      "metadata": {
        "id": "g0l6GD2H4e0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Eval Loop"
      ],
      "metadata": {
        "id": "FUSsuhZK4sLW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QBy1WIRI4sVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Inference Function"
      ],
      "metadata": {
        "id": "vskZ2bqO4y4r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gUHKQuMM4gLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Agency S22 Text Summarization",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}